\begin{frame}
	\begin{center}
		\includegraphics[width=8cm]{ressources/AlphaGoLogo}

	\end{center}
\end{frame}


\begin{frame}{AlphaGo}
	\begin{itemize}
		\item Développé par Google DeepMind
		\item En mars 2016 \textbf{AlphaGo} a battu Lee Sedol, l'un des meilleurs joueurs de Go
		\item Technologies utilisées:
		      \begin{itemize}
			      \item Apprentissage par renforcement
			      \item Réseaux de neurones profonds
			      \item Monte Carlo Tree Search
		      \end{itemize}
	\end{itemize}
\end{frame}



\begin{frame}{AlphaGo}{Composants}
	\begin{columns}[t]
		\begin{column}{.3\textwidth}
			\begin{block}{Rollout policy}
				Politique plus simple que les autres qui permet une simulation rapide du reste de la partie. Elle est amplement utilisée dans la simulation du MCTS.
			\end{block}
		\end{column}
		\begin{column}{.3\textwidth}
			\begin{block}{Policy network}
				Réseau neuronal qui renvoie une distribution de probabilité sur le coups possibles.
			\end{block}
		\end{column}
		\begin{column}{.3\textwidth}
			\begin{block}{Value Network}
				Réseau neuronal chargé de estimer la valuer d'une position, c'est-à-dire, les possibilités de gagner à partir de cette position.
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{AlphaGo}{Entraînement des Réseaux neuronales}
	\begin{center}
		\includegraphics[width=7cm]{ressources/Entrainement}
		\begin{columns}[t]
			\begin{column}{.4\textwidth}
				\begin{block}{Avec un modèle}
					Entrainement d'un réseaux supervisé (SL).
				\end{block}
			\end{column}
			\begin{column}{.4\textwidth}
				\begin{block}{Self-play}
					Apprentisage par renforcement à partir du SL en jouant des parties contre soi-même.
				\end{block}
			\end{column}
		\end{columns}

	\end{center}

\end{frame}


\begin{frame}{AlphaGo}{Apprentissage avec un modèle (SL)}

	\begin{itemize}
		\item Entraînement initial supervisé à partir de parties jouées par des professionnels (30 million de positions différentes).
		\item Le modèle doit prédire, à partir d'un état, le coup qu'un humain aurait fait.
		\item Réseau neuronal de type convolutionnel avec 13 couches.
		\item L’entraînement donne une politique $p_\sigma$, avec $\sigma$ les paramètres (les poids) du réseau.
		\item Le temps d'évaluation d'une position est de l'ordre de 3ms avec une une précision de 57.0\% sur l'ensemble d’entraînement.
	\end{itemize}

\end{frame}


\begin{frame}{AlphaGo}{Self-play}

	\begin{itemize}
		\item Structure égale à celle du SL, avec une politique $p_\rho$, avec des poids initiales égales à $\sigma$.
		\item Parties entre la version actuelle de l'agent et une version antérieure aléatoire.
		\item Système de recompense simple : +1 si victoire et -1 si défaite.
		\item Taux de réussite contre le SL : 80\%.
	\end{itemize}

\end{frame}


\begin{frame}{AlphaGo}{Value network}
	\begin{itemize}
		\item Réseau neuronal qui permet d'évaluer la \textit{qualité} d'une position.
		\item Il est entraîné à partir de la policy network.
	\end{itemize}
\end{frame}

\begin{frame}{AlphaGo}{Fast rollout policy}
	\begin{itemize}
		\item Politique de simulation rapide utilisé dans le MCTS.
		\item Entrainé avec le même modèle que le SL, mais avec une politique $p_\pi$ et une architecture beaucoup plus simple.
	\end{itemize}

\end{frame}


\begin{frame}{MCTS}

\end{frame}
