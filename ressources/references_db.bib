% Encoding: UTF-8

@Article{DuSa2003,
  author  = {Dutech, Alain and Samuelides, Manuel},
  journal = {Revue d'Intelligence Artificielle},
  title   = {Apprentissage par renforcement pour les processus décisionnels de Markov partiellement observés Apprendre une extension sélective du passé},
  year    = {2003},
  month   = {08},
  pages   = {559-589},
  volume  = {17},
  doi     = {10.3166/ria.17.559-589},
}

@Misc{silver2017,
  author        = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
  title         = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1712.01815},
  primaryclass  = {cs.AI},
}

@Article{Silver2016,
  author   = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  journal  = {Nature},
  title    = {Mastering the game of Go with deep neural networks and tree search},
  year     = {2016},
  issn     = {1476-4687},
  month    = {Jan},
  number   = {7587},
  pages    = {484-489},
  volume   = {529},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  day      = {01},
  doi      = {10.1038/nature16961},
  url      = {https://doi.org/10.1038/nature16961},
}

@Article{article,
  author  = {Chaslot, Guillaume and Uiterwijk, Jos and Bouzy, Bruno and Herik, H.},
  journal = {Proceedings of the 18th BeNeLux Conference on Artificial Intelligence},
  title   = {Monte-Carlo Strategies for Computer Go},
  year    = {2006},
  month   = {01},
}
@InProceedings{10.1007/978-3-540-75538-8_7,
author="Coulom, R{\'e}mi",
editor="van den Herik, H. Jaap
and Ciancarini, Paolo
and Donkers, H. H. L. M. (Jeroen)",
title="Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search",
booktitle="Computers and Games",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="72--83",
abstract="A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9{\texttimes}9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.",
isbn="978-3-540-75538-8"
}
@Book{,
  author = {Guillaume Maurice, Jean-Bernard Chaslot},
  title  = {Monte-Carlo Tree Search},
  year   = {2010},
  url    = {https://project.dke.maastrichtuniversity.nl/games/files/phd/Chaslot_thesis.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
